{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-RkAwVc3hjp",
        "outputId": "add33c0a-0fb8-4513-a6b1-81c92a37895f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.18 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install mediapipe opencv-python\n",
        "\n",
        "# Import necessary libraries\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "class AdvancedEyeStateDetector:\n",
        "    def __init__(self,\n",
        "                 ear_threshold=0.10,  # Adjusted threshold\n",
        "                 min_detection_confidence=0.5):\n",
        "        \"\"\"\n",
        "        Initialize Eye State Detector with improved parameters\n",
        "\n",
        "        Args:\n",
        "            ear_threshold (float): Threshold for determining eye state\n",
        "            min_detection_confidence (float): Minimum confidence for face detection\n",
        "        \"\"\"\n",
        "        # MediaPipe Face Mesh Setup\n",
        "        self.mp_face_mesh = mp.solutions.face_mesh\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "        # Configuration Parameters\n",
        "        self.EAR_THRESHOLD = ear_threshold\n",
        "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            refine_landmarks=True,\n",
        "            min_detection_confidence=min_detection_confidence\n",
        "        )\n",
        "\n",
        "        # Eye Landmark Indices (MediaPipe Face Mesh)\n",
        "        self.LEFT_EYE = [362, 385, 387, 263, 373, 380]\n",
        "        self.RIGHT_EYE = [33, 160, 158, 133, 153, 144]\n",
        "\n",
        "    def calculate_eye_aspect_ratio(self, eye_landmarks):\n",
        "        \"\"\"\n",
        "        Calculate the Eye Aspect Ratio (EAR)\n",
        "\n",
        "        Args:\n",
        "            eye_landmarks (numpy.ndarray): Eye landmark coordinates\n",
        "\n",
        "        Returns:\n",
        "            float: Eye Aspect Ratio\n",
        "        \"\"\"\n",
        "        # Vertical eye distances\n",
        "        A = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n",
        "        B = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n",
        "\n",
        "        # Horizontal eye distance\n",
        "        C = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n",
        "\n",
        "        # EAR calculation\n",
        "        ear = (A + B) / (2.0 * C)\n",
        "        return ear\n",
        "\n",
        "    def detect_eye_state(self, image):\n",
        "        \"\"\"\n",
        "        Detect eye state with improved accuracy\n",
        "\n",
        "        Args:\n",
        "            image (numpy.ndarray): Input image\n",
        "\n",
        "        Returns:\n",
        "            dict: Detailed eye state information\n",
        "        \"\"\"\n",
        "        # Convert to RGB if needed\n",
        "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            image_rgb = image\n",
        "\n",
        "        # Process image with MediaPipe\n",
        "        results = self.face_mesh.process(image_rgb)\n",
        "\n",
        "        # Default return if no face detected\n",
        "        if not results.multi_face_landmarks:\n",
        "            return {\n",
        "                'is_open': False,\n",
        "                'openness_percentage': 0.0,\n",
        "                'error': 'No face detected',\n",
        "                'visualization': image\n",
        "            }\n",
        "\n",
        "        # Extract face landmarks\n",
        "        face_landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "        # Extract eye landmarks with absolute coordinates\n",
        "        left_eye_landmarks = np.array([\n",
        "            (face_landmarks.landmark[i].x * image.shape[1],\n",
        "             face_landmarks.landmark[i].y * image.shape[0])\n",
        "            for i in self.LEFT_EYE\n",
        "        ])\n",
        "\n",
        "        right_eye_landmarks = np.array([\n",
        "            (face_landmarks.landmark[i].x * image.shape[1],\n",
        "             face_landmarks.landmark[i].y * image.shape[0])\n",
        "            for i in self.RIGHT_EYE\n",
        "        ])\n",
        "\n",
        "        # Calculate Eye Aspect Ratio for each eye\n",
        "        left_ear = self.calculate_eye_aspect_ratio(left_eye_landmarks)\n",
        "        right_ear = self.calculate_eye_aspect_ratio(right_eye_landmarks)\n",
        "\n",
        "        # Average EAR\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Determine eye state\n",
        "        is_open = avg_ear > self.EAR_THRESHOLD\n",
        "\n",
        "        # Calculate openness percentage more meaningfully\n",
        "        # Map EAR to a percentage, with thresholds\n",
        "        if avg_ear <= 0.1:\n",
        "            openness_percentage = 0.0\n",
        "        elif avg_ear >= 0.4:\n",
        "            openness_percentage = 100.0\n",
        "        else:\n",
        "            # Linear interpolation between 0.1 and 0.4\n",
        "            openness_percentage = ((avg_ear - 0.1) / 0.3) * 100\n",
        "\n",
        "        # Prepare visualization\n",
        "        output_image = image.copy()\n",
        "\n",
        "        # Left Eye Bounding Box\n",
        "        left_eye_bbox = cv2.boundingRect(left_eye_landmarks.astype(np.int32))\n",
        "        cv2.rectangle(output_image,\n",
        "                      (left_eye_bbox[0], left_eye_bbox[1]),\n",
        "                      (left_eye_bbox[0] + left_eye_bbox[2], left_eye_bbox[1] + left_eye_bbox[3]),\n",
        "                      (0, 255, 0), 2)\n",
        "\n",
        "        # Right Eye Bounding Box\n",
        "        right_eye_bbox = cv2.boundingRect(right_eye_landmarks.astype(np.int32))\n",
        "        cv2.rectangle(output_image,\n",
        "                      (right_eye_bbox[0], right_eye_bbox[1]),\n",
        "                      (right_eye_bbox[0] + right_eye_bbox[2], right_eye_bbox[1] + right_eye_bbox[3]),\n",
        "                      (0, 255, 0), 2)\n",
        "\n",
        "        # Annotate with eye state\n",
        "        eye_state = \"Open\" if is_open else \"Closed\"\n",
        "        cv2.putText(\n",
        "            output_image,\n",
        "            f\"Eyes: {eye_state} ({openness_percentage:.2f}%)\",\n",
        "            (10, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            1, (0, 255, 0), 2\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'is_open': is_open,\n",
        "            'openness_percentage': openness_percentage,\n",
        "            'visualization': output_image\n",
        "        }\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"\n",
        "        Process video and detect eye state in each frame\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to input video\n",
        "        \"\"\"\n",
        "        # Open the video capture\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Check if video opened successfully\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video file {video_path}\")\n",
        "            return\n",
        "\n",
        "        # Video output setup\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        # Output video writer\n",
        "        output_path = 'output_eye_detection.mp4'\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        # Validate output video writer\n",
        "        if not out.isOpened():\n",
        "            print(f\"Error: Could not create output video file {output_path}\")\n",
        "            cap.release()\n",
        "            return\n",
        "\n",
        "        frame_count = 0\n",
        "        eye_open_count = 0\n",
        "        eye_closed_count = 0\n",
        "\n",
        "        print(\"Starting video processing...\")\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                # Read a frame from the video\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                # Break the loop if no more frames\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Detect eye state\n",
        "                result = self.detect_eye_state(frame)\n",
        "\n",
        "                # Write processed frame to output video\n",
        "                out.write(result['visualization'])\n",
        "\n",
        "                # Track eye state statistics\n",
        "                frame_count += 1\n",
        "                if result['is_open']:\n",
        "                    eye_open_count += 1\n",
        "                else:\n",
        "                    eye_closed_count += 1\n",
        "\n",
        "                # Print progress periodically\n",
        "                if frame_count % 30 == 0:\n",
        "                    print(f\"Processed {frame_count} frames. \"\n",
        "                          f\"Current state: {'Open' if result['is_open'] else 'Closed'}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during video processing: {e}\")\n",
        "\n",
        "        finally:\n",
        "            # Release resources\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "        # Print final statistics\n",
        "        print(f\"\\nVideo Processing Complete:\")\n",
        "        print(f\"Total Frames Processed: {frame_count}\")\n",
        "        print(f\"Frames with Open Eyes: {eye_open_count}\")\n",
        "        print(f\"Frames with Closed Eyes: {eye_closed_count}\")\n",
        "        print(f\"Open Eyes Percentage: {(eye_open_count/frame_count)*100:.2f}%\")\n",
        "        print(f\"Output saved to: {output_path}\")\n",
        "\n",
        "def main():\n",
        "    # Define file path\n",
        "    file_path = \"/content/41126-427876260_small.mp4\"  # Replace with your desired path\n",
        "\n",
        "    # Validate file path\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return\n",
        "\n",
        "    # Initialize detector with adjusted threshold\n",
        "    detector = AdvancedEyeStateDetector(ear_threshold=0.25)\n",
        "\n",
        "    # Determine file type and process accordingly\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    try:\n",
        "        if file_ext in ['.png', '.jpg', '.jpeg']:\n",
        "            # Image processing\n",
        "            image = cv2.imread(file_path)\n",
        "            if image is None:\n",
        "                print(f\"Error: Could not read image from {file_path}\")\n",
        "                return\n",
        "\n",
        "            result = detector.detect_eye_state(image)\n",
        "            print(f\"Image Analysis:\")\n",
        "            print(f\"Eyes: {'Open' if result['is_open'] else 'Closed'}\")\n",
        "            print(f\"Percentage of Openness: {result['openness_percentage']:.2f}%\")\n",
        "\n",
        "            # Save visualization\n",
        "            cv2.imwrite('eye_detection_result.jpg', result['visualization'])\n",
        "            print(\"Visualization saved as 'eye_detection_result.jpg'\")\n",
        "\n",
        "        elif file_ext in ['.mp4', '.avi', '.mov']:\n",
        "            # Video processing\n",
        "            detector.process_video(file_path)\n",
        "\n",
        "        else:\n",
        "            print(\"Unsupported file type. Please use an image or video file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMJw6Jwf3tnC",
        "outputId": "04eafd76-66fb-473c-fabc-3388d7605d68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting video processing...\n",
            "Processed 30 frames. Current state: Open\n",
            "Processed 60 frames. Current state: Open\n",
            "Processed 90 frames. Current state: Open\n",
            "Processed 120 frames. Current state: Open\n",
            "Processed 150 frames. Current state: Closed\n",
            "Processed 180 frames. Current state: Open\n",
            "Processed 210 frames. Current state: Open\n",
            "Processed 240 frames. Current state: Open\n",
            "Processed 270 frames. Current state: Open\n",
            "Processed 300 frames. Current state: Open\n",
            "Processed 330 frames. Current state: Open\n",
            "\n",
            "Video Processing Complete:\n",
            "Total Frames Processed: 353\n",
            "Frames with Open Eyes: 322\n",
            "Frames with Closed Eyes: 31\n",
            "Open Eyes Percentage: 91.22%\n",
            "Output saved to: output_eye_detection.mp4\n"
          ]
        }
      ]
    }
  ]
}